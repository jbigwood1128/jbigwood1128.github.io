## Partitioning Using a Random Forest Model

Eddy covariance (EC) retrievals of net ecosystem exchange (NEE) are difficult to partition into gross primary production (GPP) and ecosystem respiration. I used a random forest model (RFM) to identify a fit residual from light response curves and compared it to the fit of a non-rectangular hyperbolic regression. The steps I took to achieve this comparison and partition NEE can be found in the report below. 

***

## Introduction 

  The global carbon flux controls much of earth’s climate but is one of the biggest unknowns in future climate projections. Tropical forests account for approximately one third of photosynthetic CO2 uptake, making them a substantial contributor to global carbon flux (Anav et al., 2015; Beer et al., 2010). Uncertainties in terrestrial carbon sequestration arise from constantly changing land use – such as deforestation and degradation – and ecosystem feedback (NASA Jet Propulsion Laboratory, 2021). One of the largest uncertainties in tropical forests is how photosynthetic productivity, often quantified as gross primary production (GPP), reacts to climate change (Lee et al., 2013). GPP is the rate of photosynthetic CO2 uptake by an ecosystem and a key observable in carbon cycle modeling (Anav et al., 2015). Quantifying GPP in the tropics is especially challenging for two main reasons: CO2 eddy covariance (EC) retrievals of net ecosystem exchange (NEE) are difficult and the separation of NEE into GPP and respiration requires additional observations (Frankenberg et al., 2011; Joiner et al., 2011). Secondly, remote sensing (RS) in the tropics has been hampered by persistent greenness and frequent cloud cover. In particular, persistent greenness makes commonly used vegetative indices unsuitable to determine GPP in the tropics. As a result, we lack robust quantitative links between GPP and RS data. This currently limits the use of satellite RS data to scale local observations and obtain GPP estimates across larger tropical areas. Due to the current limitations of vegetative RS data and its relationship to GPP, more research is necessary to address the intricate connection between photosynthetic efficiency and GPP (Magney et al., 2020).

  To address these uncertainties, I and collaborators from UCLA, UC Davis, and Costa Rica will continue to collect a comprehensive set of in-situ EC, RS, and environmental data at the well-established field site at La Selva Biological Station in La Selva, Costa Rica (10.4306° N, 84.0070° W). Figure 1 shows the comprehensive field setup we have set up at La Selva. From these instruments, data is collected and stored in google drive. The datasets necessary to partition NEE include the eddy covariance flux data, the storage data from the tower’s profile system, and the biometeorological data that contains various meteorological parameters. Combining these datasets after pre-processing and calculating the total NEE for CO2 allows for the application of a non-rectangular hyperbolic regression, which is a commonly used regression model for this type of data and NEE partitioning (Frankenberg et al., 2011). I will then compare this fit residual to a RFM using a bagging method since the data is quite noisy. This is partially due to the preprocessing specifications, such as the data timeframe. For the purposes of this project, I chose to preprocess the data in half hourly timesteps, which I discuss further in the following section. 

  Partitioning the light response curves will provide the opportunity to further analyze vegetative responses to climate forcings and identify temporal differences between wetter and dryer time periods in Costa Rica (Oberbauer et al., 2000). The results indicate GPP levels that agree with previous findings of GPP estimations in the tropics. Interestingly enough, the data supports the notion that the forest is more productive in the dry periods. Ultimately this work can be used to develop upscaling methods to create a full spatial and temporal map of photosynthesis in the tropics (Frankenberg et al., 2011, Oberbauer et al., 2000). 

## Data

  As previously noted, the data consists of three datasets, each collected separately from various instruments shown in figure 1. EC measurements are taken using an open path sonic anemometer and the gas in analyzed using an infrared gas analyzer to identify various concentration levels. The sonic anemometer sits on top of the 57m high tower. EC data, also known as the flux data since the measurements are of gas concentration fluxes, and processed using EddyPro software. 

![](assets/IMG/Fig1.png)

*Figure 1: Comprehensive field set up at La Selva Biological Station.*

  Typically, flux data is processed in one of three averaging intervals: 15-minute, half hourly, or hourly. Each time interval has its own pros and cons; for example, the 15-minute time scale does the best job of capturing quick flux measurements, which is important during early morning convective events, but in turn, it’s noisier. Hourly averages, on the other hand, are much smoother, but miss important transitions. For this reason, I chose to analyze half hourly time intervals. It’s still likely that half hourly processing neglects to pick up on small timescale transitions, but it’s much less noisy than the 15-minute interval, making it easier to work with. Within EddyPro, I chose a maximum missing sample allowance of 30% and opted into a time lag correction for my data to maximize the covariance. This means that there’s a maximum of 30% missing samples in the output file, and if the number of missing values exceeds this number, then the input file will be skipped. Lastly, to calculate the turbulent flux, EddyPro performs block averaging, or Reynolds averaging, which calculates the mean and turbulent fluctuations from the mean as mean individual departures. The EddyPro output file contains data such as the turbulent fluxes and the respective quality control flags based on the turbulence conditions (see figure 2), storage flux estimations, friction velocity, wind speed, and many other parameters. 

![](assets/IMG/Fig2.png)

*Figure 2: Turbulent fluxes for sensible heat exchange, latent heat exchange, water (which should mimic the latent heat exchange), and carbon dioxide flux. A quiality control of 0 is good data, 1 is ok data, and 2 is data that the processing software thinks should be discarded. *

  The profile data are collected from various instruments ascending the tower and help constrain the storage flux as gases enter and exit the forest canopy (see figure 3). Profile systems are especially important to accompany top-of-tower sonic anemometer and infrared gas analyzer EC systems since storage fluxes can only be estimated from above the canopy. The data collected by the profile system includes gas concentrations and temperature, shown in figure 3. Calculating the storage flux from the profile incorporates the third dataset, the biometeorological data, which includes parameters such as temperate, pressure, photosynthetically active radiation (PAR), relative humidity, etc. By integrating the gas concentrations at each level of the profile and converting them to fluxes, I can calculate the profile storage flux and compare it to the EddyPro estimated storage flux. These fluxes align well, and help validate the data, which is then added to the turbulent fluxes and plotted based on PAR in light response curves (LRCs) (see figure 4). 

![](assets/IMG/Fig3.png)![](assets/IMG/Fig3b.png)

*Figure 3: Tower profile measurements for January 2, 2023. Tower not to scale.*

![](assets/IMG/Fig4.png)

*Figure 4: Light response curves for the data span of July 2022 - May 2023. The non-rectangular hyperbolic equation is shown below the curves.*

## Modelling

  Bagging involves taking a sample of the original dataset but the data that’s randomly taken from the original dataset resets after each sample is drawn. It implies that the sunset is the same size as the original dataset. Bagging is important because it creates variability in a dataset based on the original dataset, that a machine learning model can then be applied to. The RFM contains random decision trees that form as much diversity as possible among individual trees. Each tress is trained on a different dataset thanks to bagging, and each tree can be trained on random patches, or features available at each node. These are a few of many ways to randomize the random forests. I’ll be using the decision trees for regression purposes, although they can be used for classification as well. Decision trees are easily understandable and can handle both discrete and continuous functions. Decision trees rely on a recursive algorithm and individual trees will continue growing, so they will overfit if left unsupervised. 

  First, I bagged the dataset to allow for random forest regression and used a shallow decision tree with a maximum depth of 2. Following our homework exercise in our third homework, I modeled the data from a 100 decision tree ensemble, again with a maximum depth of 2. Then, using a new maximum depth of 5, I recomputed the random forest model which I compared to the bagging data to visualize how the model overfits the data. In addition to performing the random forest regression, I played around with various other regression fits with my data, to try and identify what fits worked well, and if I could simulate the non-rectangular hyperbolic fit with a different model. 

  I chose to use these machine learning models on my data for numerous reasons. Firstly, the dataset is large and diverse. This helps the random forest algorithm learn patterns. There are also many diverse features that could be used as predictors, such as relative humidity, average temperature, maximum temperature, maximum nighttime temperature, historical monthly temperature, precipitation, vapor pressure deficit, etc. Unfortunately, I wasn’t something I had time to incorporate these predictors into my project this term, but this is something I’ll be doing in the new year. Diverse features will help the model generalize to new data. Another reason the random forests model was a good choice for this dataset was because of how it handles outliers. There are some outliers in the data, but random forests are robust to outliers, and can also handle missing values.  

## Results

Below are the results from the various regression model fits I applied to the LRCs as well as the random forest regression. 

![](assets/IMG/Fig5.png)

*Figure 5: NEE vs PAR light response curve with a linear regression.*

![](assets/IMG/Fig6.png)

*Figure 6: NEE vs PAR light response curve with a quadratic regression.*

![](assets/IMG/Fig7.png)

*Figure 7: NEE vs PAR light response curve with a non-rectangular hyberbolic regression.*

![](assets/IMG/Fig8a.png)

*Figure 8: Decison tree regression from 100 trees using a maximum depth of 2.*

![](assets/IMG/Fig9a.png)

*Figure 9: Random forest regression from 100 trees using a maximum depth of 2.*

![](assets/IMG/Fig10a.png)

*Figure 10: Random forest regression compared to a bagging regressor.*

## Discussion

  Of the linear regression, quadratic regression, and non-rectangular hyperbolic regression, the quadratic regression had the best fit with a residual of 0.41. The non-rectangular regression had a similar but slightly lower residual of 0.36 and the linear regression had a residual of 0.28. Typically, a non-rectangular regression is chosen for this data since plants are increasingly productive with more light until they reach a light saturation threshold, at which point no matter how much more light they receive, they’ve already reached peak productivity. This is because plants depend not just on light to photosynthesize, but also temperature and water availability. It’s not uncommon for plants to close their stomata – the pores on leaves that allow carbon dioxide to enter and water to escape when photosynthesizing—during peak hours of the day, since this is also around the hottest time of the day when less water is available. For this reason, it’s not surprising that the quadratic function has a larger residual, since this takes into account the small decrease on plant productivity at exceedingly high light levels. 

  In Figure 8, the non-rectangular hyperbolic fit struggles more, and the decision tree regression is very choppy since the maximum depth is only two. The decision tree regression does a better job of representing the data when the slope is close to either 1 or 0, due to the jumps, or lack thereof. The random forest regression in Figure 9 does a better job of fitting to the data, but the error is still it’s since it’s a greedy model. In figure 10 when the maximum depth is increased to 5, the random model excessively overfits the data, but the bootstrap aggregation helps limit the dataset variance by averaging subsets of the data, meaning the model is less impacted by outliers and providing a better fit. 

## Conclusion

  The tropical carbon budget is crucial to understanding terrestrial ecosystem response to various climate forcing scenarios. The response of GPP to environmental stressors in tropical rainforests is a major knowledge gap (Anav, A. et al., 2015; Frankenberg, C. et al., 2011). Since NEE data is pivotal for benchmarking terrestrial productivity, integrating machine learning models with data measurements will strengthen robustness of photosynthetic relationships over larger scales, and allow quantifying GPP and photosynthesis of the entire tropical forest. Depicting accurate photosynthetic relationships will clarify ecosystem response to climate change and reduce the uncertainties of global climate model predictions (Sun et al., 2022).

	Of the three regression fits, the quadratic fit had the highest residual due to the saturation and water stress plants encounters during the height of the day. The random forest regression overfits the data, but the bagging regressor makes the fit much smoother. Now that all months of data have been processed, a random forest model that could be used to predict future parameters ion the non-rectangular hyperbolic fit would be very useful, including how various meteorological parameters act as predictors and which parameters impact the regression the most. 

## References

[1] Anav, Alessandro, et al. “Spatiotemporal patterns of Terrestrial Gross Primary Production: A Review.” Reviews of Geophysics, vol. 53, no. 3, 2015, pp. 785–818, https://doi.org/10.1002/2015rg000483. 

[2] Beer, Christian, et al. “Terrestrial gross carbon dioxide uptake: Global distribution and covariation with climate.” Science, vol. 329, no. 5993, 2010, pp. 834–838, https://doi.org/10.1126/science.1184984. 
Effects of Climate Factors on Daytime Carbon Exchange from an Old ..., www.jstor.org/stable/41760055. Accessed 8 Dec. 2023. 

[3] Frankenberg, Christian, et al. “New global observations of the terrestrial carbon cycle from GOSAT: Patterns of plant fluorescence with gross primary productivity.” Geophysical Research Letters, vol. 38, no. 17, 2011, https://doi.org/10.1029/2011gl048738. 

[4] Joiner, J., et al. “First observations of global and seasonal terrestrial chlorophyll fluorescence from space.” Biogeosciences, vol. 8, no. 3, 2011, pp. 637–651, https://doi.org/10.5194/bg-8-637-2011. 

[5] Magney, Troy S., et al. “On the covariation of chlorophyll fluorescence and photosynthesis across scales.” Geophysical Research Letters, vol. 47, no. 23, 2020, https://doi.org/10.1029/2020gl091098. 

[6] NASA Study Finds Tropical Forests’ Ability to Absorb Carbon Dioxide Is ..., climate.nasa.gov/news/3102/nasa-study-finds-tropical-forests-ability-to-absorb-carbon-dioxide-is-waning/?trk=public_post_comment-text. Accessed 8 Dec. 2023. 

[7] Sun, Wu, et al. “Leaf relative uptake of carbonyl sulfide to CO2 seen through the lens of stomatal conductance–photosynthesis coupling.” New Phytologist, vol. 235, no. 5, 2022, pp. 1729–1742, https://doi.org/10.1111/nph.18178. 
[back](./)
