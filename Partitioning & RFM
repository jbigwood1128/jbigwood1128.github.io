## Partitioning Using a Random Forest Model

Eddy covariance (EC) retrievals of net ecosystem exchange (NEE) are difficult to partition into gross primary production (GPP) and ecosystem respiration. I used a random forest model (RFM) to identify a fit residual from light response curves and compared it to the fit of a non-rectangular hyperbolic regression. The steps I took to achieve this comparison and partition NEE can be found in the report below. 

***

## Introduction 

  The global carbon flux controls much of earth’s climate but is one of the biggest unknowns in future climate projections. Tropical forests account for approximately one third of photosynthetic CO2 uptake, making them a substantial contributor to global carbon flux (Anav et al., 2015; Beer et al., 2010). Uncertainties in terrestrial carbon sequestration arise from constantly changing land use – such as deforestation and degradation – and ecosystem feedback (NASA Jet Propulsion Laboratory, 2021). One of the largest uncertainties in tropical forests is how photosynthetic productivity, often quantified as gross primary production (GPP), reacts to climate change (Lee et al., 2013). GPP is the rate of photosynthetic CO2 uptake by an ecosystem and a key observable in carbon cycle modeling (Anav et al., 2015). Quantifying GPP in the tropics is especially challenging for two main reasons: CO2 eddy covariance (EC) retrievals of net ecosystem exchange (NEE) are difficult and the separation of NEE into GPP and respiration requires additional observations (Frankenberg et al., 2011; Joiner et al., 2011). Secondly, remote sensing (RS) in the tropics has been hampered by persistent greenness and frequent cloud cover. In particular, persistent greenness makes commonly used vegetative indices unsuitable to determine GPP in the tropics. As a result, we lack robust quantitative links between GPP and RS data. This currently limits the use of satellite RS data to scale local observations and obtain GPP estimates across larger tropical areas. Due to the current limitations of vegetative RS data and its relationship to GPP, more research is necessary to address the intricate connection between photosynthetic efficiency and GPP (Magney et al., 2020).

  To address these uncertainties, I and collaborators from UCLA, UC Davis, and Costa Rica will continue to collect a comprehensive set of in-situ EC, RS, and environmental data at the well-established field site at La Selva Biological Station in La Selva, Costa Rica (10.4306° N, 84.0070° W). Figure 1 shows the comprehensive field setup we have set up at La Selva. From these instruments, data is collected and stored in google drive. The datasets necessary to partition NEE include the eddy covariance flux data, the storage data from the tower’s profile system, and the biometeorological data that contains various meteorological parameters. Combining these datasets after pre-processing and calculating the total NEE for CO2 allows for the application of a non-rectangular hyperbolic regression, which is a commonly used regression model for this type of data and NEE partitioning (Frankenberg et al., 2011). I will then compare this fit residual to a RFM using a bagging method since the data is quite noisy. This is partially due to the preprocessing specifications, such as the data timeframe. For the purposes of this project, I chose to preprocess the data in half hourly timesteps, which I discuss further in the following section. 

  Partitioning the light response curves will provide the opportunity to further analyze vegetative responses to climate forcings and identify temporal differences between wetter and dryer time periods in Costa Rica (Oberbauer et al., 2000). The results indicate GPP levels that agree with previous findings of GPP estimations in the tropics. Interestingly enough, the data supports the notion that the forest is more productive in the dry periods. Ultimately this work can be used to develop upscaling methods to create a full spatial and temporal map of photosynthesis in the tropics (Frankenberg et al., 2011, Oberbauer et al., 2000). 

## Data

  As previously noted, the data consists of three datasets, each collected separately from various instruments shown in figure 1. EC measurements are taken using an open path sonic anemometer and the gas in analyzed using an infrared gas analyzer to identify various concentration levels. The sonic anemometer sits on top of the 57m high tower. EC data, also known as the flux data since the measurements are of gas concentration fluxes, and processed using EddyPro software. 

![](assets/IMG/Fig1.png)

*Figure 1: Comprehensive field set up at La Selva Biological Station.*

  Typically, flux data is processed in one of three averaging intervals: 15-minute, half hourly, or hourly. Each time interval has its own pros and cons; for example, the 15-minute time scale does the best job of capturing quick flux measurements, which is important during early morning convective events, but in turn, it’s noisier. Hourly averages, on the other hand, are much smoother, but miss important transitions. For this reason, I chose to analyze half hourly time intervals. It’s still likely that half hourly processing neglects to pick up on small timescale transitions, but it’s much less noisy than the 15-minute interval, making it easier to work with. Within EddyPro, I chose a maximum missing sample allowance of 30% and opted into a time lag correction for my data to maximize the covariance. This means that there’s a maximum of 30% missing samples in the output file, and if the number of missing values exceeds this number, then the input file will be skipped. Lastly, to calculate the turbulent flux, EddyPro performs block averaging, or Reynolds averaging, which calculates the mean and turbulent fluctuations from the mean as mean individual departures. The EddyPro output file contains data such as the turbulent fluxes and the respective quality control flags based on the turbulence conditions (see figure 2), storage flux estimations, friction velocity, wind speed, and many other parameters. 

![](assets/IMG/Fig2.png)

*Figure 2: Turbulent fluxes for sensible heat exchange, latent heat exchange, water (which should mimic the latent heat exchange), and carbon dioxide flux. A quiality control of 0 is good data, 1 is ok data, and 2 is data that the processing software thinks should be discarded. *

The profile data are collected from various instruments ascending the tower and help constrain the storage flux as gases enter and exit the forest canopy (see figure 3). Profile systems are especially important to accompany top-of-tower sonic anemometer and infrared gas analyzer EC systems since storage fluxes can only be estimated from above the canopy. The data collected by the profile system includes gas concentrations and temperature, shown in figure 3. Calculating the storage flux from the profile incorporates the third dataset, the biometeorological data, which includes parameters such as temperate, pressure, photosynthetically active radiation (PAR), relative humidity, etc. By integrating the gas concentrations at each level of the profile and converting them to fluxes, I can calculate the profile storage flux and compare it to the EddyPro estimated storage flux. These fluxes align well, and help validate the data, which is then added to the turbulent fluxes and plotted based on PAR in light response curves (LRCs) (see figure 4). 

![](assets/IMG/Fig3.png)![](assets/IMG/Fig3b.png)

*Figure 3: Tower profile measurements for January 2, 2023. Tower not to scale.*

![](assets/IMG/Fig4.png)

*Figure 4: Light response curves for the data span of July 2022 - May 2023. The non-rectangular hyperbolic equation is shown below the curves.*

## Modelling

Bagging involves taking a sample of the original dataset but the data that’s randomly taken from the original dataset resets after each sample is drawn. It implies that the sunset is the same size as the original dataset. Bagging is important because it creates variability in a dataset based on the original dataset, that a machine learning model can then be applied to. The RFM contains random decision trees that form as much diversity as possible among individual trees. Each tress is trained on a different dataset thanks to bagging, and each tree can be trained on random patches, or features available at each node. These are a few of many ways to randomize the random forests. I’ll be using the decision trees for regression purposes, although they can be used for classification as well. Decision trees are easily understandable and can handle both discrete and continuous functions. Decision trees rely on a recursive algorithm and individual trees will continue growing, so they will overfit if left unsupervised. 

First, I bagged the dataset to allow for random forest regression and used a shallow decision tree with a maximum depth of 2. Following our homework exercise in our third homework, I modeled the data from a 100 decision tree ensemble, again with a maximum depth of 2. Then, using a new maximum depth of 5, I recomputed the random forest model which I compared to the bagging data to visualize how the model overfits the data. In addition to performing the random forest regression, I played around with various other regression fits with my data, to try and identify what fits worked well, and if I could simulate the non-rectangular hyperbolic fit with a different model. 

## Results

Below are the results from the various regression model fits I applied to the LRCs as well as the random forest regression. 

![](assets/IMG/Fig2.png)

*Figure 5: Comprehensive field set up at La Selva Biological Station.*

Figure X shows... [description of Figure X].

## Discussion

From Figure X, one can see that... [interpretation of Figure X].

## Conclusion

Here is a brief summary. From this work, the following conclusions can be made:
* first conclusion
* second conclusion

Here is how this work could be developed further in a future project.

## References
[1] DALL-E 3

[back](./)
